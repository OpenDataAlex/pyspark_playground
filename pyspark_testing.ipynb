{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions\n\nspark \u003d SparkSession.builder.master(\"local\").appName(\"Neural Network Model\").config(\"spark.executor.memory\", \"6gb\").getOrCreate()\n\nsc \u003d spark.sparkContext\n\nwords \u003d sc.parallelize([\n    \u0027scala\u0027,\n    \u0027java\u0027,\n    \u0027hadoop\u0027,\n    \u0027spark\u0027,\n    \u0027akka\u0027,\n    \u0027spark vs hadoop\u0027,\n    \u0027pyspark\u0027,\n    \u0027pyspark and spark\u0027\n])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Number of elements: 8\nElements in RDD [\u0027scala\u0027, \u0027java\u0027, \u0027hadoop\u0027, \u0027spark\u0027, \u0027akka\u0027, \u0027spark vs hadoop\u0027, \u0027pyspark\u0027, \u0027pyspark and spark\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "counts \u003d words.count()\nprint (\"Number of elements: %i\" % counts)\n\ncoll \u003d words.collect()\nprint(\"Elements in RDD %s\" % coll)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Filtered RDD -\u003e [\u0027spark\u0027, \u0027spark vs hadoop\u0027, \u0027pyspark\u0027, \u0027pyspark and spark\u0027]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def f(x): print(x)\nfore \u003d words.foreach(f)\n\nwords_filter \u003d words.filter(lambda x: \u0027spark\u0027 in x)\nfiltered \u003d words_filter.collect()\nprint(\"Filtered RDD -\u003e %s\" % filtered)\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "words_map \u003d words.map(lambda x: (x, 1))\nmapping \u003d words_map.collect()\nprint(\"Key value pair -\u003e %s\" % mapping)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Key value pair -\u003e [(\u0027scala\u0027, 1), (\u0027java\u0027, 1), (\u0027hadoop\u0027, 1), (\u0027spark\u0027, 1), (\u0027akka\u0027, 1), (\u0027spark vs hadoop\u0027, 1), (\u0027pyspark\u0027, 1), (\u0027pyspark and spark\u0027, 1)]\n"
          ],
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Adding all the elements -\u003e 15\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from operator import add\nnums \u003d sc.parallelize([1, 2, 3, 4, 5])\nadding \u003d nums.reduce(add)\nprint(\"Adding all the elements -\u003e %i\" % adding)\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Join RDDs -\u003e [(\u0027hadoop\u0027, (4, 5)), (\u0027spark\u0027, (1, 2))]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "sc \u003d spark.sparkContext\nx \u003d sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\ny \u003d sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\njoined \u003d x.join(y)\nfinal \u003d joined.collect()\nprint(\"Join RDDs -\u003e %s\" % final)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Accumulated value is -\u003e 150\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "sc \u003d spark.sparkContext\nnum \u003d sc.accumulator(10)\n\ndef f(x):\n    global num\n    num+\u003dx\n    \nrdd \u003d sc.parallelize([20,30,40,50])\nrdd.foreach(f)\nfinal \u003d num.value\nprint(\"Accumulated value is -\u003e %i\" % final)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}